---
layout: post
title: "Overlapping Gardens"
published: 2024-01-12
updated: 2024-01-14
status: "draft"
tags: 
- software
- design
- internet
---

<section>
	<table-of-contents>
		<h4>Page Contents</h4>
		<ol>
			<li><a href="#overview">Caught in the Web</a></li>
			<li><a href="#xanadu">Ted Nelson's Xanadu</a></li>
			<li><a href="#collaboration">Collaboration</a></li>
			<li><a href="#annotation">Non-linearity</a></li>
			<li><a href="#curation">Curation</a></li>
		</ol>
	</table-of-contents>
	<!-- <top-note>This is a rough research document</top-note> -->

	<h3><a id="overview">Caught in the Web</a></h3>
	<p>
		I grew up with the web. From bopping around AOL chatrooms in the mid-90s, to 
		geocities sites and webrings
		custom html sites on whatever free hosting i could find
		ftp servers
		cd burning
		bit torrent sharing
		
	</p>
	<p>
		In my view, 2023 was the final year for current crop of social media. We are, once again, at a crossroads, where we will most likely jump to the thing that closest resembles the world we know. But I think we should stop for a while, look around, give more thought to where we want to go, and what we want technology to do for us. Given that the whole of society has become caught up in this wild web of ours, I believe it's a worthwhile exercise.
	</p>
	<p>
		Now, this isn't a piece on the negative effects of social media. I believe that diagnosis has already been thoroughly made.
	</p>
	<p>
		When typing up my notes for Is This Writing?, my good pal reminded me of Mike Caulfield's piece, <a href="https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/">The Garden and the Stream: A Technopastoral</a>. I find the garden analogy very compelling (partly because I'm typing this with ten green fingers). Tending to a little plot of infospace, connecting and growing ideas sounds refreshing when compared to the frenzy of the stream. 
	</p>
	<p>
		What I find even more appealing is the potential for overlapping gardens. A social web that isn't just about Posting. [...]
	</p>	
	<p></p>
	<h3><a id="xanadu">Ted Nelson's Xanadu</a></h3>
	<h4>Overview</h4>

	<blockquote>
		"The original hypertext concept of the 1960s got lost on the way to the Web-- and all current document standards oppose it."
	</blockquote>

	<p>
		The structure of paper lacks depth and support for non-linearity. HyperText and the Internet have immitated this conventional media of the past. Xanadu attempts to advance document standards to escape from the prison of paper. 
	</p>
	<p>
		Videos of Ted talking about and demonstrating Xanadu:	
	</p>
	<ul>
		<li>
			<a href="https://www.youtube.com/watch?v=hMKy52Intac">Xanadu Basics 1a - VISIBLE CONNECTION</a>
		</li>
		<li>
			<a href="https://www.youtube.com/watch?v=1gPM3GqjMR4">Xanadu Basics 1b-- INDIRECT DOCUMENT DELIVERY</a>
		</li>
	</ul>

	<h4>Key concepts</h4>

	<ul>
		<li>
			Parallelism
			<ul>
				<li>Visible connections -- two way links</li>
				<li>Marginal notes</li>
				<li>Side by side viewing</li>
				<li>Many links</li>
				<li>Attribution. See the origins of content</li>
			</ul>
		</li>
		<li>
			Transclusion
			<ul><li>Same content in multiple places</li></ul>
		</li>
		<li>
			Generalized media format
			<ul>
				<li>Any combination of media formats </li>
				<li>Floating links (flinks!?)</li>
				<li>Edit in place</li>
			</ul>
		</li>
	</ul>

	<h4>Notes</h4>
	<p>
		We see "cheapened" version of these things throughout the internet today:
	</p>
	<ul>
<!-- 		<li>
			Parallelism
			<ul>
				<li>Visible connections -- two way links</li>
				<li>Marginal notes</li>
				<li>Side by side viewing</li>
				<li>Many links</li>
				<li>Attribution. See the origins of content</li>
			</ul>
		</li> -->
		<li>
			Two-way links
			<ul>
				<li>
					<a href="https://en.wikipedia.org/wiki/Linkback">Linkbacks</a>
					<ul>
						<li><a href="https://en.wikipedia.org/wiki/Pingback">Pingback</a></li>
						<li><a href="https://en.wikipedia.org/wiki/Trackback">Trackback</a></li>
					</ul>
				</li>
				
			</ul>
			<li>
			</li>
			<li>
			</li>
		</li>
	</ul>
	<p>
		In the most basic sense, knowing which document links to your own would add another dimension to the internet. The graph that arrises from these connections <em>is</em> gettable now, through automated internet crawling, but I like the idea of it being available as a core concept of the web. As I understand it, this is the graph that search engines create, traverse, and (page)rank in order to present us with the most relevant web pages to our search.<span class="callout">&midast;</span><span class="sidenote"><span class="callout">&midast;</span> Is this still how they work? I wonder how connected sites are in the web today, where most surfers stick to walled gardens on social media islands. [Some time later] I watched a whole dang <a href="https://www.youtube.com/watch?v=tFq6Q_muwG0"> corporate propaganda "home movie"</a> about this and still have no idea.</span> While that is useful, making the graph searchable flattens it from multiple dimensions-- every document can have one, many, or zero connections to every other docunent-- down to one dimension: a list of sites most relevant to the search term (which most of us just use for the top item). The connectedness is lost to the black box of the engine.
	</p>
	<p>
		What I take away from Ted's decades-long obsession with the two-way links is that he's trying to imagine traversing that graph in a different way. Here are a few that come to mind:
	</p>
	<ul>
		<li>
			<b>Link awareness:</b> You're looking into DIY solar energy systems (which I find myself doing often) and you happen upon a site you really like. So, you link to it from your site. Because this is a two-way street ::hand wave:: the author can now see that you've done that and choose to link back to your document and start a conversation.
		</li>
		<li>
			<b>Transclusion (embedding but better)</b> There's a segment on the site that does a better job explaining the basics of batteries than you could ever do, so you <em>transclude</em> it in the research document you're compiling-- much like the quotes on this document, except it is still tied to the source and maybe even keeps track of changes when they occur.
		</li>
		<li>
			<b>Neighbors:</b> You like the battery site and want to find other sites that someone might find when doing this research. In your browser, you jump to an interface that shows a view of the graph around it [To-do: Diagram] and explore the site's neighbors.
		</li>
		<li>
			<b>Paths:</b> Patterns of traffic flow (much like the trail of tabs that come from an internet rabbit hole) will emerge in these two-way links. A few different paths might emerge from the battery site. Maybe some people arrived there following a path for off-grid cabins, and others are more interested in marine applications. Many paths intersect at this site, but might continue on in different directions. Your browser could help you navigate these paths through the graph without the help of a search engine.
		</li>
	</ul>
	<p>
		Again, technically, these scenarios can happen now through a constant rereading and recataloging (spidering) of the web. But that's such a boneheaded way to go about it. If the links on a website were two-way, those connections could be served with each pageload and navigable as part of the browsing experience. The connections would be a part of the web. We would interact with a search engine less or in a different way.

		It's hard to say how much transcluding peolpe would do because the way it happens now is mostly with copy and paste or one-way linking. 
	</p>
	<p>
		Overall, I think Ted has taken the visible connections concept too literally for general use. In Xanadu Basics 1a, he presents a series of texts (like the Talmud) to show how people have made connections visible in text throughout the years. While this was meant to be evidence for the need for the three-dimensional interface he shows later, I don't find it convincing for something like the general-purpose internet. Couldn't a less heavy interface do the job? Perhaps it would be good for comparing two versions of the same document, as Ted shows. But, other examples he mentions-- comments, annotations, details(?), summaries, alternatives, outlines, table of contents, even footnotes-- are all things that CAN be displayed on the side of an HTML document with a simple design change. This document has those things. What I could see is 
	</p>
	<p>
		Bringing real attribution into this copyright-happy, rent-seeking, hyper-litigious world might not be a good thing. In a world where ownership was not consolidated and monetized, I could see it being a fantastic way to understand where something originated so you could find more things like it, or thank and support the creator. Maybe open source software is the model here, not the RIAA.
	</p>
	<h4>Related Readings</h4>
	<ul>
		<li><a href="https://xanadu.com/xuTheModel/index.html">DEEP HYPERTEXT: THE XANADU® MODEL</a></li>
		<li><a href="https://readwrite.com/ted_nelsons_two/">Ted Nelson’s two-way links</a></li>	
		<li><a href="https://web.archive.org/web/20071009230444/http://www.disenchanted.com/dis/technology/xanadu.html">Ghosts of Xanadu</a></li>
		<blockquote>“Cheap and democratic as it was, Berners-Lee’s Web didn’t have half the features Xanadu promised to, and two-way linking was one of them. Without a central server it couldn’t be enforced<span class="callout">&midast;</span><span class="sidenote"><span class="callout">&midast;</span> Why is this the case? What does the central server do that server-to-server communication couldn't?</span> and to make authorship of pages as simple as possible – given the state of the art at the time – it had to be left out along with automatic attribution, micropayments, copyright management, unbreakable links, and most of Nelson’s other ideas. But ten years later, a ghost of Xanadu is being recreated in the same style as the Web itself: quick, dirty, and cheap. Like Xanadu, it could have interesting implications for the way we structure knowledge.”</blockquote>
		<blockquote>The referrer has the potential to turn everyday web surfers into a kind of resource discovery brigade. In ant colonies, soldier ants leave a trail of pheromones back to the source of any food they've discovered so the rest of the hive can find it, too. And each successive ant that follows the trail will reinforce it with their own scent. To get the same effect with Backlinks you just count how many visitors arrived by each inbound pathway. Sort your list of referrers this way and the visitor can tell at a glance where the best “food” must be.</blockquote>
	</ul>
	<h3><a id="curation">Curation</a></h3>
	<p>
		In a complete 180, what if it's a mistake to try to organize the world's information algorithmically? What if discovery is better done by humans? How can we better enable curation? 
	</p>
	<!-- <p>
		I just watched the whole <a href="https://www.youtube.com/watch?v=tFq6Q_muwG0"> corporate propaganda "home movie"</a> from Google on how search works:
	</p>
	<ul>
		<li>They won't tell us how search works for fear of people gaming it</li>
		<li>Something about content analysis</li>
		<li>Probably still ranked by a link graph</li>
		<li>An army of (likely very low paid) humans test the quality of the search algorithm</li>
	</ul>
	<p>
		Ultimately, humans are the judge of quality, even in the biggest, most automated information organization undertaking. I understand that it's probably just a representative sample of the full range of searches that are tested by humans, but why would we want to create links where
	</p> -->
	<h3><a id="collaboration">Collaboration</a></h3>
	<h3><a id="annotation">Annotation and Non-linearity</a></h3>
</section>